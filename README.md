# LLM-fullstack
A Fullstack application using ollama, flask, postgre and next.js. The purpose is to have a chatbot that uses RAG with my personal obsidian knowledgebase. 

### TODOs
- [x] Create MVP with prompts and responses
- [x] Have a working chat with chat history
- [x] Create Vector database with pgvector and create module for Retrieval Augmented Generation (RAG)
- [x] Style Interface a little bit
- [X] Markdown correctly rendered
- [ ] Create component to embed obsidian knowlegdebase
- [ ] Secure Inputs (page changes or closes when user prompts something, while generating user should not make another prompt)
- [X] Add Options to each chat for e.g. adding context
- [X] Add Option to delete chats
- [ ] Dockerize application for fast deployment

![Example Image of the Chat with a LLM agent.](/ExampleChat.png)
